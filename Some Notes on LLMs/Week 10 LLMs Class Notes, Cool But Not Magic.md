Rationalism: we need the rationalism of a computer to bring us utopia. **But computers are ultimately a creation of people, so they aren't rational because they were coded by non rational beings. If computers sprung magically out of the ground, this might work.**

'Good AI' is meant to push the world forward towards progress. Anyone who is against this is 'the wrong type of person' -> basis in eugenics and a very theistic approach towards computing. CEO of Google basically thinks that climate change is pointless to fix now, we might as well burn more oil to get the computers to fix it for us.

But neural networks...aren't that powerful??? In the form they're in now??? Why are we hyping this so much??
### AI is literally terrible and I don't want any part in any of this, make it stop!!

AI weirdos basically start to think (because of success in training models on large data sets) that we need to train AI on the largest datasets possible. But, models work very well on small, curated datasets work very well -- much better than the big models that are basically just autocorrect made by eugenicists.

Using these models is not straightforward anymore because we're using humans to train them up on several layers. There are so many layers of value added into these models that there is no way that you could ever argue that they are rational or separate from human values (and a very specific, nasty set of human values).

**Can't create anything new, can only draw from the preset version of pixel values.**

**Eliza Effect:** Computing guy writes a program called Eliza, that takes the person's input, looks for key phrases that were hard-coded in, and it signals a Rogerian psychoanalysis that basically just repeats back everything back to you that you say to it. This leads to people developing parasocial relationships with something that has no intelligence or intentionality -> obv this is really bad!!
- AI has a context window that shifts with the conversation, which basically means that if the conversation continuously gets darker, eventually that will be all that the model knows, which creates a really awful feedback loop
We tried out Semantle, interesting because the pathway seems fairly arbitrary because they're working in so many dimensions. This is where the illusion of creativity comes from, because we can't always trace the path that the model is taking.
### God, intern, or cog?
If it's a god, walk away, you're going to become part of the eugenicist cult.
If it's an intern, you'd be bouncing ideas off them and trust them not to make mistakes in their area of expertise.
If it's a cog, it's a small model deployed for a particular purpose.
#### Cog is probably the best bet here -> for example, a useful cog might be a research partner that looks over your writing and gives you suggestions. If you give your model that identity, maybe it could help you, right?
Sort of a generated version of rubber ducking, where you can explain to the model what you're doing and it will give you some responses back on how you can strengthen your writing. 

## The only reason any of this works is because of the massive data sets, the enormity of vectors, and the fantasy of the Eliza effect. It's actually kind of stupid. Why are we treating this like it's the be-all end-all of everything?
(See above. Eugenics and incels.)






